{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdea7dd3",
   "metadata": {},
   "source": [
    "* * *\n",
    "<pre> NYU Paris            <i> Artificial intelligence - Summer 2023</i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> Lab 10: Q-learning </h1>\n",
    "\n",
    "<pre align=\"left\"> June 30th 2022               <i> Author: Hicham Janati </i></pre>\n",
    "* * *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cf71b",
   "metadata": {},
   "source": [
    "Q-learning is a classical example of reinforcement learning where an agent learns from its environment by taking actions and learning from rewards and penalties. For the sake of simplicity consider the following environment where the agent's initial state is at the Start position. It can take actions (left, right, up, down). The game ends either when it's game-over (Death cell) or the agent reaches the Exit. Each instance of a game is called an episode. The agent plays a certain number of episodes (i.e training) and continuously updates the actions it should take depending on which state it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_maze():\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    xs = np.linspace(0.5, 5.5, 4)\n",
    "    ys = np.linspace(0.5, 5.5, 4)\n",
    "    s = 11\n",
    "    ax.plot(xs, ys, color=\"white\")\n",
    "    ax.hlines(1, 1, 5, color=\"k\", lw=3)\n",
    "    ax.vlines(1, 1, 5, color=\"k\", lw=3)\n",
    "    ax.hlines(5, 1, 5, color=\"k\", lw=3)\n",
    "    ax.vlines(5, 1, 5, color=\"k\", lw=3)\n",
    "    ax.text(1.05, 1.4, \"Start\", color=\"navy\", fontsize=s, alpha=0.8)\n",
    "    ax.text(4.05, 2.4, \"Exit(+20)\", color=\"darkgreen\", fontsize=s, alpha=1.)\n",
    "    ax.text(2.05, 2.4, \"Fire (-10)\", color=\"red\", fontsize=s, alpha=0.8)\n",
    "    ax.text(4.05, 4.4, \"Gold (+5)\", color=\"forestgreen\", fontsize=s, alpha=0.8)\n",
    "    ax.text(4.05, 3.4, \"Trap (-5)\", color=\"red\", fontsize=s, alpha=0.8)\n",
    "    ax.text(1.05, 4.4, \"Gold (+10)\", color=\"forestgreen\", fontsize=s, alpha=0.8)\n",
    "    ax.text(2.05, 4.4, \"Trap (-2)\", color=\"red\", fontsize=s, alpha=0.8)\n",
    "    ax.text(2.05, 3.4, \"Gold (+2)\", color=\"forestgreen\", fontsize=s, alpha=0.8)\n",
    "    ax.text(2.05, 1.4, \"Death (-20)\", color=\"darkred\", fontsize=s, alpha=0.8)\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "plot_maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f44826",
   "metadata": {},
   "source": [
    "There are 16 possible states in this environment. At each state, the agent can take 4 actions. We can represent the \"quality\" of a pair (state, action) i.e taking action a while being at state s in a matrix Q (16 x 4) that we initialize to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa12322",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Create an array of size (16, 2) with all the state coordinates using the square grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e76b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(1.5, 4.5, 4)\n",
    "\n",
    "state_positions = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef9212",
   "metadata": {},
   "source": [
    "The rewards of each state: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([0, -20, 0, 0, 0, -10, 0, 20, 0, 2, 0, -5, 10, -2, 0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360d5ed",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "We use the convention: 0 = Left, 1 = Down, 2 = Right, 3 = Up. \n",
    "Implement a function `get_possible_actions` that takes an int (0-15) (id of an element of `state_positions`) and returns a list of possible actions. This function must **only** make sure the next action remains within the boundary of the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_steps = np.array([[-1, 0], [0, -1], [0, 1], [0, 1]])\n",
    "\n",
    "def get_possible_actions(state_id):\n",
    "    state = state_positions[state_id]\n",
    "\n",
    "    return actions\n",
    "\n",
    "get_possible_actions(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2115fe6",
   "metadata": {},
   "source": [
    "The process goes as follows. Let's assume the agent is at state $s_t$.\n",
    "\n",
    "1. To take an action, the agent can either:\n",
    "- explore: select a random action among the allowed actions (cannot go outside the maze)\n",
    "- or exploit: select an action maximizing the quality of the pair-actions possible (i.e using the Q table)\n",
    "\n",
    "The main idea of Q-learning is to decide randomly between explore / exploit with a tendency to explore more in the beginning and exploit at the end. Let epsilon be the probability of deciding to explore. We initialize epsilon = 1 and decrease gradually after each episode. \n",
    "\n",
    "2. Once the action $a_t$ is taken, the agent is now in $s_{t+1}$ and has obtained a reward $r_t$ (the new cell score) which can be used to update the quality of taking $a_t$ while being in the (previous) state $s_t$:\n",
    "\n",
    "$$ Q(s_t, a_t) = Q(s_t, a_t) + \\alpha (r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)) $$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate (weight given to how much correction should be applied at each step)\n",
    "- $\\gamma \\in [0,1] $  is the 'discount' rate, which lowers the importance of 'future' rewards compared to immediate ones. It is usually taken around 0.95-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65378ef1",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Complete the code below that performs the training of the agent. To avoid infinite loops, we set a max number of steps per episode (game) to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "max_steps = 30\n",
    "rng = np.random.default_rng(42)\n",
    "epsilon = 1.\n",
    "learning_rate = 0.1\n",
    "Q = np.zeros((16, 4))\n",
    "discount_rate = 0.99\n",
    "decay_rate = 0.99\n",
    "# training\n",
    "training_paths = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = 0\n",
    "    path = [0]\n",
    "    for s in range(max_steps):\n",
    "        \n",
    "        # decide to explore or exploit\n",
    "        action = \n",
    "        # take the action to get a new state\n",
    "        new_state = \n",
    "        \n",
    "        \n",
    "        path.append(new_state)\n",
    "\n",
    "        # Update quality given the new reward\n",
    "        reward = \n",
    "        Q[state,action] = Q[state,action] + learning_rate * (reward + discount_rate * np.max(Q[new_state, :])-Q[state, action])\n",
    "    \n",
    "        # if gameover or won, finish episode\n",
    "        if :\n",
    "            break\n",
    "    \n",
    "    training_paths.append(path)\n",
    "    # Decrease epsilon\n",
    "    epsilon = np.exp(-decay_rate*episode)\n",
    "    print(f\"epsilon = {epsilon}\")\n",
    "\n",
    "print(f\"Training completed over {num_episodes} episodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d7c9e",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Use the matplotlib code above, visualize the paths of your agent as it is learning. Does it converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd366dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed8edf24",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Visualize the evolution of the Q matrix using a plt.imshow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d81af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e337738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
