{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86ff817",
   "metadata": {
    "id": "d86ff817"
   },
   "source": [
    "* * *\n",
    "<pre> NYU Paris            <i> Artificial intelligence - Summer 2023 </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> Lab 6: Machine learning for music </h1>\n",
    "\n",
    "<pre align=\"left\"> June 7th 2023              <i> Author: Hicham Janati </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "##### Goals:\n",
    "- Create digital sound with Python\n",
    "- Introduction to signal processing (Fourier transform)\n",
    "- Use machine learning for classification on music data\n",
    "- Discover deep learning with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rngfLWw_KosD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rngfLWw_KosD",
    "outputId": "02875b97-fa19-4d66-c05f-5562b41c3342"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dIIjbz6K61c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dIIjbz6K61c",
    "outputId": "0501c9fa-4bdd-4009-ca20-ff171d34c108"
   },
   "outputs": [],
   "source": [
    "music_path = \"drive/MyDrive/colab_data/resampled-audio/genres\"\n",
    "!ls drive/MyDrive/colab_data/resampled-audio/genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff0f65",
   "metadata": {
    "id": "cfff0f65"
   },
   "source": [
    "## Part 1:  basics of digital sound processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc86e85",
   "metadata": {
    "id": "8dc86e85"
   },
   "source": [
    "Most signals in the real world are presented in analog format (electromagnetic / acoustic waves). To process them in a computer, we need a digital converter. An analog signal corresponding to playing the note `A` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912e893",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "2912e893",
    "outputId": "947497fb-b578-4120-cc43-86897cdef779"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frequency = 440\n",
    "w = 2 * np.pi * frequency \n",
    "time_points = np.linspace(0., 1., 100000)\n",
    "sine_wave = np.sin(w * time_points)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time_points[:1000], sine_wave[:1000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094f9ea",
   "metadata": {
    "id": "7094f9ea"
   },
   "source": [
    "A digital converter samples from this wave a certain number of points decided by the `sampling_rate` parameter. A high sampling rate is required for a good conversion of audio signals but naturally leads to more data storage requirements. For most audio systems, a sampling rate of 44100 is enough. In the example below, we use a lower sampling rate for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c47fc99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "6c47fc99",
    "outputId": "5f3c8d2a-b356-432b-f8d0-60c72820851b"
   },
   "outputs": [],
   "source": [
    "def plot_sampling(frequency, sampling_rate, duration=1):\n",
    "    N_full = int(duration * 100000)\n",
    "    time_points_full = np.linspace(0., 1., N_full)\n",
    "\n",
    "    N = int(duration * sampling_rate)\n",
    "    time_points = np.linspace(0., 1., N)\n",
    "\n",
    "    w = 2 * np.pi * frequency \n",
    "\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(time_points_full, np.sin(w * time_points_full), lw=2, color=\"gray\", label=\"Analog signal\")\n",
    "    ax.scatter(time_points, np.sin(w * time_points), s=15, color=\"indianred\", label=\"Digital samples\")\n",
    "    ax.plot(time_points, np.sin(w * time_points), color=\"indianred\", ls=\"-.\", alpha=0.5)\n",
    "\n",
    "    ax.set_xlim([0., 0.01])\n",
    "    plt.legend(ncol=2, bbox_to_anchor=[0.1, 1.05])\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "\n",
    "plot_sampling(frequency, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0eaa5",
   "metadata": {
    "id": "add0eaa5"
   },
   "source": [
    "### Question 1\n",
    "Using the function `plot_sampling` defined above, investigate the effect of `sampling_rate`. Select a fixed frequency of 500 and decrease the sampling rate from 10000 to 200 progressively. What do you conclude ? \n",
    "Can you guess where the choice of 44100 Hz might come from ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f5ee5",
   "metadata": {
    "id": "687f5ee5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0c6adb",
   "metadata": {
    "id": "ba0c6adb"
   },
   "source": [
    "We wrap the code above in a nice function that makes a single tone sound wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b7dc0",
   "metadata": {
    "id": "ac8b7dc0"
   },
   "outputs": [],
   "source": [
    "def make_mono_freq(frequency, duration=3, sampling_rate=44100, amplitude=16000):\n",
    "    \"\"\"Creates a single-frequency sound wave with duration in seconds\"\"\"\n",
    "    N = int(sampling_rate * duration)\n",
    "    w = 2 * np.pi * frequency \n",
    "    time_points = np.arange(N) / sampling_rate\n",
    "    sine_wave = amplitude * np.sin(w * time_points)\n",
    "    return sine_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33ac63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "da33ac63",
    "outputId": "2bc68853-48b8-4389-92f5-57ddc3e9b35f"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "frequency = 1000\n",
    "sampling_rate = 44100\n",
    "duration = 2\n",
    "sound = make_mono_freq(frequency, duration=duration, sampling_rate=sampling_rate)\n",
    "print(\"Shape of sound data:\", sound.shape)\n",
    "Audio(data=sound, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666980af",
   "metadata": {
    "id": "666980af"
   },
   "source": [
    "Given the signal S, the discrete Fourier transform computes an \"importance score\" or weight of each frequency in the signal. Using the sound wave above, the Fourier transform should return zero for all frequencies except 200 since it's the only present in our signal. Our signal has real values (no complex values) we use the _Real Fast Fourier Transform (rfft)_. RFFT outputs symetric (+ and - freq), we take their absolute value using `np.abs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa110832",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "fa110832",
    "outputId": "82783ab3-4e63-4159-e3d8-25b0dffbd60e"
   },
   "outputs": [],
   "source": [
    "from scipy.fft import rfft, rfftfreq\n",
    "\n",
    "all_frequencies = rfftfreq(sampling_rate * duration, 1 / sampling_rate)\n",
    "fft_data = rfft(sound)\n",
    "fft_weights = np.abs(fft_data)\n",
    "plt.plot(all_frequencies, fft_weights)\n",
    "plt.ylabel(\"FFT weights\")\n",
    "plt.xlabel(\"Frequencies\")\n",
    "plt.xlim([0, 5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ef236",
   "metadata": {
    "id": "783ef236"
   },
   "source": [
    "Extract the most interesting frequencies by truncating the weights above a small threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61b35a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a61b35a",
    "outputId": "708a5e1f-a2fe-4d5c-f545-213d036b86d6"
   },
   "outputs": [],
   "source": [
    "important_freq_indx = np.where(fft_weights > 0.01)\n",
    "important_freq = all_frequencies[important_freq_indx]\n",
    "important_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626dd65",
   "metadata": {
    "id": "5626dd65"
   },
   "source": [
    "We recovered the frequency we used to construct our signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "km3l3zbeWg64",
   "metadata": {
    "id": "km3l3zbeWg64"
   },
   "source": [
    "### Question 1\n",
    "Read and understand what does the function below then use it to create an up-beat melody with 10 frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6a413",
   "metadata": {
    "id": "99d6a413"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_multi_freq(frequencies, duration=1):\n",
    "    duration_ = duration / len(frequencies)\n",
    "    waves = [make_mono_freq(f, duration_) for f in frequencies]\n",
    "    waves = np.concatenate(waves)\n",
    "    return waves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pez3kAzVWtSS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "Pez3kAzVWtSS",
    "outputId": "31b45aac-99db-435d-9e2e-ade7b85a5963"
   },
   "outputs": [],
   "source": [
    "sound = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5aed6b",
   "metadata": {
    "id": "be5aed6b"
   },
   "source": [
    "### Question 2\n",
    "Contaminate this sound with a noisy high pitch tone. Use a frequency of 5000 for example and 1% of the amplitude of the clean signal. Display the wave of this signal and listen to it. Play with its amplitude to control how noisy the sum of boths signals gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955856f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "3955856f",
    "outputId": "3d1b2cde-be0f-4009-c732-3d5591768afd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cad0d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "90cad0d4",
    "outputId": "4d9f4c02-7616-49b7-eec7-637d6dc7e80f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c298c7",
   "metadata": {
    "id": "a8c298c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6bd0595",
   "metadata": {
    "id": "f6bd0595"
   },
   "source": [
    "### Question 3\n",
    "Apply the fast fourier transform on the obtained signal. In the same scipy package you will find the _Inverse real fast Fourier transform_ `irfft` which does the inverse transformation (frequency domain -> time domain). Using `irfft`, perform a denoising operation to recover the clean uncontaminated signal from the noisy one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d1d05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "f50d1d05",
    "outputId": "8b8062bd-3cce-41f1-a7bc-8b30d25cab12"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e996a",
   "metadata": {
    "id": "a85e996a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "724d6340",
   "metadata": {
    "id": "724d6340"
   },
   "source": [
    "### Question 4\n",
    "While the FFT spectrum of most signals summarizes the data in a very compact manner and provides valuable information, it loses the temporal information of the signal. To keep both, we can run `FFT` over small a small temporal window that we move progressively thus \"scanning\" the audio signal. This is called _Short term Fourier transform_. Visualize the spectrogram for the sound, contaminated and denoised signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708bab0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "b708bab0",
    "outputId": "e0116c28-b9c1-45be-e47a-07ef1be248fd"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "X = librosa.stft(sound)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=44100, x_axis='time', y_axis='hz')\n",
    "plt.ylim([0, 6000])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10a42",
   "metadata": {
    "id": "e8b10a42"
   },
   "source": [
    "The spectrogram data can be saved as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b3f9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "f86b3f9e",
    "outputId": "6060e056-9225-42b4-b6ea-25365f3b1261"
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('inferno')\n",
    "melspect = librosa.feature.melspectrogram(sound, sr=22050)\n",
    "plt.imshow(melspect, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad468d",
   "metadata": {
    "id": "f8ad468d"
   },
   "source": [
    "# Part 2: music genre classification\n",
    "The purpose of the second part of this lab is to perform music genre classification using the spectrogram of the audio signals. We create a dataset of spectrogram images from the audio files. To save time, I generated all the spectrograms beforehand and saved them as numpy .npy files. The following cell reads them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2ba40",
   "metadata": {
    "id": "b8d2ba40",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "X = []\n",
    "y = []\n",
    "genres = [\"classical\", \"jazz\", \"rock\", \"reggae\", \"blues\", \"metal\", \"pop\", \"disco\", \"hiphop\", \"country\"]\n",
    "for label, genre in enumerate(genres):\n",
    "    for filename in os.listdir(f'{music_path}/{genre}/'):\n",
    "        if filename.split('.')[-1] == 'npy':\n",
    "            melspect = np.load(f'{music_path}/{genre}/{filename}')\n",
    "            X.append(melspect)\n",
    "            y.append(label)\n",
    "        if filename.split('.')[-1] == 'au':\n",
    "          pass\n",
    "          # this is an audio file\n",
    "          # it can be read using:\n",
    "          # sound, sr = librosa.load(f\"{music_path}/{genre}/{filename}\", mono=True)\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X /= X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0KywqPFZb3F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0KywqPFZb3F",
    "outputId": "e8bcb5fe-8d01-4cc3-d07a-3b258a192c5b"
   },
   "outputs": [],
   "source": [
    "print(X.shape, X.min(), X.max(), np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQ-skzV_3HNx",
   "metadata": {
    "id": "RQ-skzV_3HNx"
   },
   "outputs": [],
   "source": [
    "X_flat = X.reshape(len(X), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3601197",
   "metadata": {
    "id": "b3601197"
   },
   "source": [
    "### Question 5\n",
    "To have an idea -- and satisfy your curiosity -- start by loading an audio file of each genre and listen to it. Use the function `librosa.load`. See how its used in the commented portion above.\n",
    "\n",
    "Then display the average spectrogram of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bd3bc",
   "metadata": {
    "id": "ab1bd3bc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "JMLQJsO08KUD",
   "metadata": {
    "id": "JMLQJsO08KUD"
   },
   "source": [
    "\n",
    "### Question 6\n",
    "Perform a PCA(2) on the the data and visualize the samples with their color-coded labels. What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qs72Nz979Ebi",
   "metadata": {
    "id": "Qs72Nz979Ebi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "v3jppmns7hew",
   "metadata": {
    "id": "v3jppmns7hew"
   },
   "source": [
    "\n",
    "### Question 7\n",
    "Perform a train-test split and run a logistic regression. Do a simple cross-validation on 1 hyperparameter. How does the train/test performance indicate ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KKhFTAsI8DWu",
   "metadata": {
    "id": "KKhFTAsI8DWu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7Gj5hfKi6lRr",
   "metadata": {
    "id": "7Gj5hfKi6lRr"
   },
   "source": [
    "### Question 8\n",
    "Same question with an SVM classifier. Do two separate gridsearches over 'C' each with a fixed kernel ('linear' and 'rbf). How does the choice of the kernel impact the performance ? Was it expected ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GiYzIMOe6kj3",
   "metadata": {
    "id": "GiYzIMOe6kj3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c3fbd41",
   "metadata": {
    "id": "3c3fbd41"
   },
   "source": [
    "We change the format of the data to torch tensors and perform a train test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a673438",
   "metadata": {
    "id": "8a673438"
   },
   "source": [
    "### Part 3: introduction to deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d105b1a",
   "metadata": {
    "id": "5d105b1a"
   },
   "source": [
    "\n",
    "With pytorch, a neural netwrok is an boject that inherist from `nn.Module`. It must contain:\n",
    "\n",
    "- a constructor ```___init___``` which specifies its attributes (number of layers, neurones etc)\n",
    "- a forward function ```forward``` that computes the output of the neural net given a data input `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20733340",
   "metadata": {
    "id": "20733340"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"Simple neural net.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear_layer = nn.Linear(X.shape[1] * X.shape[2], 3) \n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.linear_layer(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f6f3b",
   "metadata": {
    "id": "794f6f3b"
   },
   "source": [
    "### Question 9\n",
    "What model does this first neural net above correspond to ?\n",
    "\n",
    "\n",
    "After creating our neural network, we need to write a function that performs the 'fit' operation i.e solve the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e70ac",
   "metadata": {
    "id": "383e70ac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_neural_net(neural_net,\n",
    "                   X_train, X_test, y_train, y_test,\n",
    "                   learning_rate=0.05,\n",
    "                   weight_decay=0.,\n",
    "                   max_iter=1000,\n",
    "                   device='cpu',\n",
    "                   verbose=True):\n",
    "    \"\"\"\n",
    "    Function that performs back-propagation (gradient descent)\n",
    "    to optimize the neural net.\n",
    "    neural_net: Instance of NeuralNet\n",
    "    learning_rate: gradient step size\n",
    "    weight_decay: regularization parameter of L2 regularization\n",
    "    max_iter: nombre maximum d'itérations\n",
    "    device: 'cpu' or 'cuda:0'\n",
    "\n",
    "    Returns\n",
    "    scores: dictionary of s\n",
    "    \"\"\"\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss().to(device)\n",
    "    neural_net = neural_net.to(device)\n",
    "    optimizer = optim.Adam(neural_net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    loss_train, loss_test = [], []\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    X_train = X_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    if verbose:\n",
    "        strings = [s.center(15) for s in [\"Iteration\", \"Accuracy Train\", \"Accuracy Test\", \"Loss Train\", \"Loss Test\"]]\n",
    "        strings = [s.center(15) for s in strings]\n",
    "        strings = \" | \".join(strings)\n",
    "        print(strings)\n",
    "\n",
    "    for ii in range(max_iter):\n",
    "        # Train\n",
    "        neural_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_output = neural_net(X_train)\n",
    "        loss_train_ = loss_func(train_output, y_train)  #### compute the loss\n",
    "        loss_train_.backward()  ### compute the gradients and do back propagation\n",
    "        optimizer.step()    # gradient step\n",
    "\n",
    "\n",
    "      # Test\n",
    "      # turn neural net to evaluation mode to not keep computing gradients\n",
    "        neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            _, y_pred_train = torch.max(train_output.data, 1)\n",
    "            test_output = neural_net(X_test.to(device))\n",
    "            _, y_pred_test = torch.max(test_output.data, 1)\n",
    "            loss_test_ = loss_func(test_output, y_test).cpu().item()\n",
    "            accuracy_test_ = (y_pred_test == y_test).float().mean().cpu().item() \n",
    "            accuracy_train_ = (y_pred_train == y_train).float().mean().cpu().item() \n",
    "\n",
    "        if verbose and ii % 100 == 0:\n",
    "            strings = [ii, accuracy_train_, accuracy_test_, loss_train_.item(), loss_test_] # print stuff\n",
    "            strings = [\"{:.4f}\".format(s).center(15) for s in strings]\n",
    "            strings = \" | \".join(strings)\n",
    "            print(strings)\n",
    "\n",
    "        loss_train.append(loss_train_.cpu().item())\n",
    "        loss_test.append(loss_test_)\n",
    "        accuracy_train.append(accuracy_train_)\n",
    "        accuracy_test.append(accuracy_test_)\n",
    "\n",
    "    metrics = [[loss_train, loss_test],\n",
    "              [accuracy_train, accuracy_test]]\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def make_report(neural_net, X_test, device=\"cpu\"):\n",
    "    neural_net.eval()\n",
    "    y_pred = neural_net(X_test.to(device)).detach().cpu().numpy().argmax(1)\n",
    "    print(classification_report(y_test.cpu().numpy(), y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fXCpEGMdompn",
   "metadata": {
    "id": "fXCpEGMdompn"
   },
   "source": [
    "### Question 10\n",
    "First we need to turn our data to torch tensors. Make sure your data is reshaped properly (i.e of shape (n_samples, n_features)) and not in images shapes (n_samples, width, height).  \n",
    "\n",
    "On torch, we can move data and objects from CPU to GPU using the `to` command. Depending on how many GPUs you have, you can switch an array X to GPU number 0 by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-bPtODF0_HSS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bPtODF0_HSS",
    "outputId": "95da68c9-8f54-4ac5-9787-dc34efd3ad22"
   },
   "outputs": [],
   "source": [
    "A = torch.ones(1)\n",
    "A = A.to(\"cuda:0\")\n",
    "A.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VSxWXDiQ_UNm",
   "metadata": {
    "id": "VSxWXDiQ_UNm"
   },
   "source": [
    "and back to cpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pSqISqot_Wij",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSqISqot_Wij",
    "outputId": "76b1313b-f43a-4806-93cf-894cfddb7d34"
   },
   "outputs": [],
   "source": [
    "A = A.to(\"cpu\")\n",
    "A.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-8_Ywiye_R9F",
   "metadata": {
    "id": "-8_Ywiye_R9F"
   },
   "source": [
    "\n",
    "In this lab, these device switches are done within the train function which accepts an argument `device`. Train the model on cpu and generate a classification report. How did it perform ?\n",
    "\n",
    "\n",
    "The function `fit_neural_net` outputs metrics (loss, accuracy) for both train and test. You can use its output in the cell below. What do you conclude ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e725338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "6e725338",
    "outputId": "08e72b1c-6ffd-4c82-c27a-67d81106da8b"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "for ax, metric, metric_name in zip(axes, OUTPUT_OF_FIT, ['Loss', 'Accuracy']):\n",
    "    for curve, color, data_type in zip(metric, ['cornflowerblue', 'indianred'], ['Train', 'Test']):\n",
    "        ax.plot(curve, color=color, label=data_type)\n",
    "        ax.set_title(metric_name)\n",
    "        ax.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TA6HcjiZAFbU",
   "metadata": {
    "id": "TA6HcjiZAFbU"
   },
   "source": [
    "### Question 11 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CUryPKLHqZuW",
   "metadata": {
    "id": "CUryPKLHqZuW"
   },
   "source": [
    "We add a first hidden layer with 100 neurones and a non-linearity (ReLU activation function). What's the consequence of this decision ? Increase the number of neurones of the hidden layer. Does it impact the result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZNOwS4RQqAzV",
   "metadata": {
    "id": "ZNOwS4RQqAzV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetHidden(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetHidden, self).__init__()\n",
    "        self.linear_layer = nn.Linear(128 * 130, 100) \n",
    "        self.hidden_layer = nn.Linear(100, 10) \n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.linear_layer(X)\n",
    "        X = torch.relu(X)\n",
    "        X = self.hidden_layer(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2z7M27bmAu__",
   "metadata": {
    "id": "2z7M27bmAu__"
   },
   "source": [
    "Reducing overffiting with neural nets is not an easy task. But there are several ways to do it:\n",
    "- Add regularization on the neural weights:\n",
    "L2 regularization can be controlled by the `weight_decay` parameter of `fit_neural_net`.\n",
    "- Add more data (bonus). Which can be done using data augmentation techniques i.e transforming the samples you already have to add more samples. Transformation should however be carefully chosen depending on the task.\n",
    "- Add Dropout layers: dropout cancels some neurones (randomly) to reduce the complexity of the model.\n",
    "- Add batch normalization layers. \n",
    "- Reduce the overall depth and complexity of the neural net.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0Bt3riJAuPc",
   "metadata": {
    "id": "d0Bt3riJAuPc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Oy5p4E18B2yP",
   "metadata": {
    "id": "Oy5p4E18B2yP"
   },
   "source": [
    "### Part 4 (Bonus): Convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15rflSH3q0W7",
   "metadata": {
    "id": "15rflSH3q0W7"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Basic ConvNet\"\"\"\n",
    "\n",
    "    def __init__(self, n_outputs=10, debug=False):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(14880, 100)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.fc2 = nn.Linear(100, n_outputs)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        # flatten over channel, height and width = 1600\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lnD0CebGCAdg",
   "metadata": {
    "id": "lnD0CebGCAdg"
   },
   "source": [
    "A convolution neural net (CNN) is a special design of a neural network for computer vision applications. Since our spectrograms can be seen as images, it is not a bad idea to use a CNN for music genre classification. \n",
    "\n",
    "First we need to reshape images to have a number of \"channels\". Here we have only 1 channel (no colors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_NjNqLz2CrGs",
   "metadata": {
    "id": "_NjNqLz2CrGs"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(len(X_train), 1, *X[0].shape)\n",
    "X_test = X_test.reshape(len(X_test), 1, *X[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2h7zhTXdD5no",
   "metadata": {
    "id": "2h7zhTXdD5no"
   },
   "source": [
    "Train this conv net and investigate its performance and complexity. Can you beat simple non-deep models ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wK2hnKx9wIbL",
   "metadata": {
    "id": "wK2hnKx9wIbL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
